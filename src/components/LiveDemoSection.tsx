'use client'

import { useState, useRef, useCallback } from 'react'

interface Message {
  type: 'user' | 'ai'
  text: string
  audioUrl?: string
  confidence?: number
  model?: string
  timestamp: Date
}

interface SurveyState {
  currentStep?: string
  nextStep?: string
  progressPercentage?: number
  isCompleted?: boolean
  answers?: Record<string, any>
}

export default function LiveDemoSection() {
  const [isListening, setIsListening] = useState(false)
  const [isAiSpeaking, setIsAiSpeaking] = useState(false)
  const [conversation, setConversation] = useState<Message[]>([])
  const [currentStep, setCurrentStep] = useState('waiting') // waiting, listening, processing, speaking
  const [transcript, setTranscript] = useState('')
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [error, setError] = useState('')
  const [currentSessionId, setCurrentSessionId] = useState<string | null>(null)
  const [surveyState, setSurveyState] = useState<SurveyState>({})
  const [isInConversation, setIsInConversation] = useState(false)

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)

  // ì„¸ì…˜ ID ìƒì„± í•¨ìˆ˜
  const generateSessionId = () => {
    return `session_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
  }

  // ë¸Œë¼ìš°ì €ë³„ ìµœì  MIME íƒ€ì… ì„ íƒ
  const getBestMimeType = () => {
    const types = [
      'audio/webm;codecs=opus',  // Chrome, Edge
      'audio/ogg;codecs=opus',    // Firefox
      'audio/mp4',                // Safari
      'audio/wav'                 // Fallback
    ]

    for (const type of types) {
      if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported(type)) {
        return type
      }
    }
    return 'audio/webm'
  }

  // WebM/Opusë¥¼ WAVë¡œ ë³€í™˜
  const convertToWav = async (webmBlob: Blob) => {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 })
    const arrayBuffer = await webmBlob.arrayBuffer()

    try {
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)

      // WAV í—¤ë” ìƒì„±
      const length = audioBuffer.length
      const buffer = new ArrayBuffer(length * 2 + 44)
      const view = new DataView(buffer)

      // WAV í—¤ë” ì‘ì„±
      const writeString = (offset: number, string: string) => {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i))
        }
      }

      writeString(0, 'RIFF')
      view.setUint32(4, length * 2 + 36, true)
      writeString(8, 'WAVE')
      writeString(12, 'fmt ')
      view.setUint32(16, 16, true) // fmt chunk size
      view.setUint16(20, 1, true) // PCM format
      view.setUint16(22, 1, true) // mono
      view.setUint32(24, 16000, true) // sample rate
      view.setUint32(28, 32000, true) // byte rate (16000 * 2)
      view.setUint16(32, 2, true) // block align
      view.setUint16(34, 16, true) // bits per sample
      writeString(36, 'data')
      view.setUint32(40, audioBuffer.length * 2, true)

      // PCM ë°ì´í„° ì‘ì„±
      const channelData = audioBuffer.getChannelData(0)
      let offset = 44
      for (let i = 0; i < channelData.length; i++) {
        const sample = Math.max(-1, Math.min(1, channelData[i]))
        view.setInt16(offset, sample * 0x7FFF, true)
        offset += 2
      }

      return new Blob([buffer], { type: 'audio/wav' })
    } catch (error) {
      console.error('WAV ë³€í™˜ ì‹¤íŒ¨:', error)
      return webmBlob // ë³€í™˜ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    }
  }

  // ì‹¤ì œ ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    if (!streamRef.current) return

    try {
      const mimeType = getBestMimeType()
      const options = { mimeType }
      const recordingStartTime = Date.now()

      mediaRecorderRef.current = new MediaRecorder(streamRef.current, {
        ...options,
        audioBitsPerSecond: 128000
      })
      audioChunksRef.current = []

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = async () => {
        const recordingDuration = Date.now() - recordingStartTime
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })

        console.log('Audio size:', audioBlob.size, 'bytes')
        console.log('Recording duration:', recordingDuration, 'ms')
        console.log('Audio chunks count:', audioChunksRef.current.length)

        // ìµœì†Œ ë…¹ìŒ ì‹œê°„ 1ì´ˆë¡œ ì¦ê°€
        if (recordingDuration < 1000) {
          setError('ë…¹ìŒ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 1ì´ˆ ì´ìƒ ë§ì”€í•´ì£¼ì„¸ìš”.')
          setCurrentStep('waiting')
          return
        }

        // ì˜¤ë””ì˜¤ í¬ê¸° ì²´í¬ (ìµœì†Œ 5KB)
        if (audioBlob.size < 5000) {
          setError('ë…¹ìŒëœ ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë§ˆì´í¬ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
          setCurrentStep('waiting')
          return
        }

        // ì„œë²„ë¡œ ì „ì†¡
        await sendToServer(audioBlob)
      }

      mediaRecorderRef.current.start(100)
      setIsRecording(true)
      setError('')
      console.log('ë…¹ìŒ ì‹œì‘ë¨')
    } catch (err) {
      setError('ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      console.error(err)
      setCurrentStep('waiting')
    }
  }, [])

  // ì„œë²„ë¡œ ì „ì†¡ ë° ì‘ë‹µ ì²˜ë¦¬
  const sendToServer = async (audioBlob: Blob) => {
    setCurrentStep('processing')
    setIsProcessing(true)
    setError('')

    try {
      // WAVë¡œ ë³€í™˜
      const wavBlob = await convertToWav(audioBlob)

      console.log('WAV file size:', wavBlob.size, 'bytes')

      if (wavBlob.size < 1000) {
        console.error('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤.')
        setError('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
        setIsProcessing(false)
        setCurrentStep('waiting')
        return
      }

      const formData = new FormData()
      formData.append('file', wavBlob, 'recording.wav')
      formData.append('language', 'ko-KR')

      // ì„¸ì…˜ ID ì¶”ê°€
      if (currentSessionId) {
        formData.append('sessionId', currentSessionId)
        console.log('Using session ID:', currentSessionId)
      }

      // API í˜¸ì¶œ
      const response = await fetch('/api/voice-assistant', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        const errorData = await response.json()
        console.error('ì„œë²„ ì˜¤ë¥˜:', errorData)

        // Show user-friendly error message from API
        setError(errorData.error || `ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (${response.status})`)
        setCurrentStep('waiting')
        setIsProcessing(false)
        return
      }

      // JSON ì‘ë‹µ ì²˜ë¦¬ (ìƒˆë¡œìš´ ì‘ë‹µ êµ¬ì¡°)
      const data = await response.json()
      console.log('API ì‘ë‹µ:', data)

      // ì„œë²„ì—ì„œ ìƒì„±í•œ ì„¸ì…˜ ID ì €ì¥ (ì²« ìš”ì²­ ì‹œ)
      if (!currentSessionId && data.sessionId) {
        setCurrentSessionId(data.sessionId)
        console.log('Received new session ID from server:', data.sessionId)
      }

      // ì„¤ë¬¸ ìƒíƒœ ì—…ë°ì´íŠ¸
      if (data.surveyState) {
        setSurveyState({
          currentStep: data.surveyState.currentStep,
          nextStep: data.surveyState.nextStep,
          progressPercentage: data.surveyState.progressPercentage,
          isCompleted: data.surveyState.isCompleted,
          answers: data.surveyState.answers
        })
        console.log('Survey state updated:', data.surveyState)
      }

      // Base64 ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ Blobìœ¼ë¡œ ë³€í™˜
      const audioBytes = atob(data.audioData)
      const audioArray = new Uint8Array(audioBytes.length)
      for (let i = 0; i < audioBytes.length; i++) {
        audioArray[i] = audioBytes.charCodeAt(i)
      }
      const responseAudioBlob = new Blob([audioArray], { type: 'audio/mpeg' })
      const audioUrl = URL.createObjectURL(responseAudioBlob)

      // ì‹¤ì œ í…ìŠ¤íŠ¸ë¡œ ë©”ì‹œì§€ ì¶”ê°€
      const userMessage: Message = {
        type: 'user',
        text: data.sttResult?.transcribedText || 'ìŒì„± ì…ë ¥',
        confidence: data.sttResult?.confidence,
        timestamp: new Date()
      }

      const aiMessage: Message = {
        type: 'ai',
        text: data.llmResponse?.message || 'AI ì‘ë‹µ',
        audioUrl: audioUrl,
        model: data.llmResponse?.model,
        timestamp: new Date()
      }

      setConversation(prev => [...prev, userMessage, aiMessage])

      // ìŒì„± ìë™ ì¬ìƒ
      setCurrentStep('speaking')
      setIsAiSpeaking(true)
      const audio = new Audio(audioUrl)
      audio.onended = () => {
        setIsAiSpeaking(false)
        setCurrentStep('waiting')
      }
      await audio.play()

    } catch (err) {
      setError(err instanceof Error ? err.message : 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤')
      console.error('ì „ì†¡ ì‹¤íŒ¨:', err)
      setCurrentStep('waiting')
    } finally {
      setIsProcessing(false)
    }
  }

  const startConversation = async () => {
    // ì„œë²„ê°€ ì„¸ì…˜ IDë¥¼ ìë™ ìƒì„±í•˜ë„ë¡ ë³€ê²½
    // ì²« ìš”ì²­ì—ì„œëŠ” sessionIdë¥¼ ë³´ë‚´ì§€ ì•ŠìŒ
    setCurrentSessionId(null)
    setIsInConversation(true)
    setSurveyState({})
    console.log('Starting new conversation, server will generate session ID')
    // í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œë§Œ ì‹¤í–‰
    if (typeof window === 'undefined') {
      console.error('Server side - cannot access browser APIs')
      setError('ì„œë²„ ì‚¬ì´ë“œì—ì„œëŠ” ë§ˆì´í¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      return
    }

    // ë” ì•ˆì „í•œ navigator API ì²´í¬
    if (typeof navigator === 'undefined') {
      setError('ë¸Œë¼ìš°ì € í™˜ê²½ì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
      console.error('Navigator not available')
      return
    }

    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      setError('ìŒì„± ë…¹ìŒì„ ì‚¬ìš©í•˜ë ¤ë©´ HTTPS í™˜ê²½ì´ í•„ìš”í•©ë‹ˆë‹¤. http://localhost:3000 ìœ¼ë¡œ ì ‘ì†í•˜ê±°ë‚˜ HTTPSë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.')
      console.error('MediaDevices not available - HTTPS required')
      return
    }

    // HTTPS í™•ì¸ - IP ì£¼ì†ŒëŠ” localhostë¡œ ì•ˆë‚´
    const isSecureContext = location.protocol === 'https:' ||
                           location.hostname === 'localhost' ||
                           location.hostname === '127.0.0.1'

    if (!isSecureContext) {
      setError('ìŒì„± ë…¹ìŒì„ ì‚¬ìš©í•˜ë ¤ë©´ http://localhost:3000 ìœ¼ë¡œ ì ‘ì†í•˜ê±°ë‚˜ HTTPSë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.')
      console.error('Secure context required for getUserMedia')
      return
    }

    try {
      console.log('ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...')

      // ê¸°ë³¸ ì˜¤ë””ì˜¤ ì œì•½ ì¡°ê±´ (í‘œì¤€ ì†ì„±ë“¤) - í’ˆì§ˆ í–¥ìƒ
      const audioConstraints: any = {
        channelCount: 1,        // ëª¨ë…¸
        sampleRate: 16000,      // 16kHz (Google STT ê¶Œì¥)
        sampleSize: 16,         // 16-bit
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
        volume: 1.0            // ìµœëŒ€ ë³¼ë¥¨
      }

      // Chrome ë¸Œë¼ìš°ì €ì—ì„œë§Œ Google ì „ìš© ì†ì„± ì¶”ê°€
      const isChrome = /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor)
      if (isChrome) {
        // Chrome ì „ìš© ì†ì„±ë“¤ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€
        Object.assign(audioConstraints, {
          googEchoCancellation: true,
          googAutoGainControl: true,
          googNoiseSuppression: true,
          googHighpassFilter: true
        })
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: audioConstraints
      })

      console.log('ë§ˆì´í¬ ê¶Œí•œ íšë“ ì„±ê³µ')
      streamRef.current = stream
      setError('')

      // í™˜ì˜ ë©”ì‹œì§€
      setConversation([{
        type: 'ai',
        text: 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë§ì”€í•´ì£¼ì„¸ìš”.',
        timestamp: new Date()
      } as Message])

      // ì¦‰ì‹œ ë…¹ìŒ ì‹œì‘
      setCurrentStep('listening')
      await startRecording()

    } catch (err: any) {
      console.error('getUserMedia error:', err)

      if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
        setError('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
        setError('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'NotReadableError' || err.name === 'TrackStartError') {
        setError('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì•±ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'OverconstrainedError' || err.name === 'ConstraintNotSatisfiedError') {
        setError('ë§ˆì´í¬ ì„¤ì •ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'TypeError' || !navigator.mediaDevices) {
        setError('HTTPS ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. https:// ë˜ëŠ” localhostì—ì„œ ì ‘ì†í•´ì£¼ì„¸ìš”.')
      } else {
        setError('ë§ˆì´í¬ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ' + (err.message || err.name || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'))
      }
    }
  }

  const startListening = async () => {
    // ìƒˆë¡œìš´ ë…¹ìŒ ì‹œì‘
    if (streamRef.current) {
      setCurrentStep('listening')
      await startRecording()
    }
  }

  const stopConversation = () => {
    // ë…¹ìŒ ì¤‘ë‹¨í•˜ê³  API í˜¸ì¶œ
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
    } else {
      setCurrentStep('waiting')
    }
  }

  const resetDemo = () => {
    // ìƒˆë¡œìš´ ì„¤ë¬¸ ì‹œì‘ - ì„œë²„ê°€ ìƒˆ ì„¸ì…˜ ID ìƒì„±í•˜ë„ë¡ nullë¡œ ì„¤ì •
    setCurrentSessionId(null)
    setConversation([])
    setCurrentStep('waiting')
    setIsListening(false)
    setIsAiSpeaking(false)
    setTranscript('')
    setSurveyState({})
    setIsInConversation(false)
    console.log('Reset demo - server will generate new session ID')
  }

  const getStatusText = () => {
    if (isRecording) return 'ğŸ¤ ë…¹ìŒ ì¤‘...'
    switch (currentStep) {
      case 'listening': return 'ğŸ¤ ë“£ê³  ìˆì–´ìš”...'
      case 'processing': return 'ğŸ¤– ë¶„ì„ ì¤‘...'
      case 'speaking': return 'ğŸ—£ï¸ AI ì‘ë‹µ ì¤‘...'
      default: return 'ëŒ€í™” ì¤€ë¹„ ì™„ë£Œ'
    }
  }

  // ì •ë¦¬ í•¨ìˆ˜
  const cleanup = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current = null
    }
    setIsRecording(false)
    setIsProcessing(false)
    setCurrentStep('waiting')
  }

  return (
    <section id="demo" className="live-demo-section">
      <div className="container">
        <div className="demo-title">
          <h2>VOTICA AI <span className="gradient-text">ì‹¤ì œ ì‹œì—°</span></h2>
          <p>ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ AIì™€ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”</p>
        </div>

        <div className="live-demo-container">
          <div className="live-demo-card">
            <div className="demo-header">
              <div className="demo-info">
                <h3>ì‹¤ì‹œê°„ AI ìŒì„± ëŒ€í™”</h3>
                <p>AIê°€ ì§ˆë¬¸í•˜ë©´ ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”</p>
              </div>

              <div className="live-indicator">
                <div className={`live-dot ${isRecording || isAiSpeaking ? 'active' : ''}`}></div>
                {getStatusText()}
              </div>
            </div>

            {/* ì„¤ë¬¸ ì§„í–‰ ìƒíƒœ */}
            {surveyState.progressPercentage !== undefined && (
              <div className="mb-4 p-3 bg-blue-500/10 border border-blue-500/30 rounded-lg">
                <div className="flex items-center justify-between text-blue-400 text-sm mb-2">
                  <span>ğŸ“Š ì„¤ë¬¸ ì§„í–‰ ì¤‘: {surveyState.currentStep || 'ì‹œì‘'}</span>
                  <span>{surveyState.progressPercentage.toFixed(1)}%</span>
                </div>
                <div className="w-full bg-blue-900/30 rounded-full h-2">
                  <div
                    className="bg-gradient-to-r from-blue-500 to-blue-400 h-2 rounded-full transition-all duration-500"
                    style={{ width: `${surveyState.progressPercentage}%` }}
                  />
                </div>
                {surveyState.nextStep && (
                  <div className="text-blue-400/70 text-xs mt-2">
                    ë‹¤ìŒ ë‹¨ê³„: {surveyState.nextStep}
                  </div>
                )}
                {surveyState.isCompleted && (
                  <div className="text-green-400 text-sm mt-2 font-semibold">
                    âœ… ì„¤ë¬¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ê°ì‚¬í•©ë‹ˆë‹¤.
                  </div>
                )}
              </div>
            )}

            {/* ì—ëŸ¬ ë©”ì‹œì§€ */}
            {error && (
              <div className="mb-4 p-3 bg-red-500/10 border border-red-500/30 rounded-lg text-red-400 text-sm">
                {error}
              </div>
            )}

            {/* ëŒ€í™” ê¸°ë¡ */}
            <div className="conversation-area">
              {conversation.length === 0 ? (
                <div className="conversation-placeholder">
                  <div className="placeholder-icon">ğŸ’¬</div>
                  <p>ëŒ€í™” ì‹œì‘ì„ ëˆŒëŸ¬ AIì™€ ì—¬ë¡ ì¡°ì‚¬ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”</p>
                </div>
              ) : (
                <div className="conversation-history">
                  {conversation.map((message, index) => (
                    <div key={index} className={`message ${message.type}`}>
                      <div className="message-avatar">
                        {message.type === 'ai' ? 'ğŸ¤–' : 'ğŸ‘¤'}
                      </div>
                      <div className="message-content">
                        <div className="message-text">{message.text}</div>
                        {message.audioUrl && (
                          <button
                            onClick={() => {
                              const audio = new Audio(message.audioUrl)
                              audio.play()
                            }}
                            className="mt-2 p-1 rounded-lg bg-white/10 hover:bg-white/20 transition-colors"
                          >
                            ğŸ”Š ë‹¤ì‹œ ë“£ê¸°
                          </button>
                        )}
                        <div className="message-time">
                          {message.timestamp.toLocaleTimeString()}
                        </div>
                      </div>
                    </div>
                  ))}
                </div>
              )}

              {isRecording && (
                <div className="live-transcript">
                  <span className="transcript-label">ë…¹ìŒ ì¤‘:</span>
                  <span className="transcript-text">ğŸ¤ ë§ì”€í•´ì£¼ì„¸ìš”...</span>
                </div>
              )}
            </div>

            {/* ì œì–´ ë²„íŠ¼ */}
            <div className="demo-controls">
              <div className="control-buttons">
                {currentStep === 'waiting' && conversation.length === 0 && (
                  <button
                    onClick={startConversation}
                    className="demo-button primary large"
                  >
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M8 5v14l11-7z"/>
                    </svg>
                    ëŒ€í™” ì‹œì‘
                  </button>
                )}

                {currentStep === 'waiting' && conversation.length > 0 && (
                  <button
                    onClick={startListening}
                    className="demo-button mic"
                    disabled={isListening || isAiSpeaking}
                  >
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                      <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                    </svg>
                    ìŒì„±ìœ¼ë¡œ ë‹µë³€
                  </button>
                )}

                {currentStep === 'listening' && (
                  <button
                    onClick={stopConversation}
                    className="demo-button secondary"
                  >
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M6 6h12v12H6z"/>
                    </svg>
                    ì¤‘ì§€
                  </button>
                )}

                {(currentStep === 'processing' || currentStep === 'speaking') && (
                  <div className="audio-visualizer">
                    <div className="wave-container">
                      {[...Array(6)].map((_, i) => (
                        <div key={i} className="wave-bar" style={{ animationDelay: `${i * 0.1}s` }}></div>
                      ))}
                    </div>
                    <p className="playing-text">{getStatusText()}</p>
                  </div>
                )}

                {conversation.length > 0 && (
                  <button
                    onClick={resetDemo}
                    className="demo-button tertiary"
                    disabled={isListening || isAiSpeaking}
                  >
                    {surveyState.isCompleted ? 'ìƒˆ ì„¤ë¬¸ ì‹œì‘' : 'ìƒˆë¡œ ì‹œì‘'}
                  </button>
                )}
              </div>
            </div>

            <div className="demo-features">
              <div className="feature-item">
                <div className="feature-icon">ğŸ¤</div>
                <div className="feature-text">
                  <strong>Google STT</strong>
                  <span>ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜</span>
                </div>
              </div>
              <div className="feature-item">
                <div className="feature-icon">ğŸ§ </div>
                <div className="feature-text">
                  <strong>ChatGPT</strong>
                  <span>ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìƒì„±</span>
                </div>
              </div>
              <div className="feature-item">
                <div className="feature-icon">ğŸ”Š</div>
                <div className="feature-text">
                  <strong>ElevenLabs TTS</strong>
                  <span>ìì—°ìŠ¤ëŸ¬ìš´ AI ìŒì„±</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  )
}
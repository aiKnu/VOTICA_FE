'use client'

import { useState, useRef, useEffect } from 'react'

export default function LiveDemoSection() {
  const [isListening, setIsListening] = useState(false)
  const [isAiSpeaking, setIsAiSpeaking] = useState(false)
  const [conversation, setConversation] = useState([])
  const [currentStep, setCurrentStep] = useState('waiting') // waiting, listening, processing, speaking
  const [transcript, setTranscript] = useState('')

  const recognitionRef = useRef(null)

  useEffect(() => {
    // Web Speech API ì´ˆê¸°í™”
    if (typeof window !== 'undefined' && 'webkitSpeechRecognition' in window) {
      recognitionRef.current = new window.webkitSpeechRecognition()
      recognitionRef.current.continuous = false
      recognitionRef.current.interimResults = false
      recognitionRef.current.lang = 'ko-KR'

      recognitionRef.current.onresult = (event) => {
        const transcript = event.results[0][0].transcript
        setTranscript(transcript)
        handleUserResponse(transcript)
      }

      recognitionRef.current.onerror = (event) => {
        console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error)
        setIsListening(false)
        setCurrentStep('waiting')
      }

      recognitionRef.current.onend = () => {
        setIsListening(false)
      }
    }
  }, [])

  const startConversation = async () => {
    // ëŒ€í™” ì´ˆê¸°í™” ë° AI ì²« ì§ˆë¬¸ ì‹œì‘
    setConversation([])
    setCurrentStep('speaking')
    setIsAiSpeaking(true)

    const firstQuestion = "ì•ˆë…•í•˜ì„¸ìš”. ì •ë¶€ ì •ì±…ì— ëŒ€í•œ êµ­ë¯¼ ì˜ê²¬ì„ ìˆ˜ë ´í•˜ê³  ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì •ë¶€ì˜ ê²½ì œ ì •ì±…ì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?"

    // ëŒ€í™” ê¸°ë¡ì— AI ì§ˆë¬¸ ì¶”ê°€
    setConversation([{ type: 'ai', text: firstQuestion, timestamp: new Date() }])

    // ì—¬ê¸°ì— ì‹¤ì œ TTS API í˜¸ì¶œì´ ë“¤ì–´ê°ˆ ì˜ˆì •
    // í˜„ì¬ëŠ” ì‹œë®¬ë ˆì´ì…˜
    setTimeout(() => {
      setIsAiSpeaking(false)
      setCurrentStep('waiting')
    }, 3000)
  }

  const startListening = () => {
    if (recognitionRef.current && !isListening) {
      setCurrentStep('listening')
      setIsListening(true)
      setTranscript('')
      recognitionRef.current.start()
    }
  }

  const handleUserResponse = async (userText) => {
    setCurrentStep('processing')

    // ì‚¬ìš©ì ì‘ë‹µì„ ëŒ€í™”ì— ì¶”ê°€
    setConversation(prev => [...prev, { type: 'user', text: userText, timestamp: new Date() }])

    // AI ì‘ë‹µ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
    setTimeout(async () => {
      setCurrentStep('speaking')
      setIsAiSpeaking(true)

      // ê°„ë‹¨í•œ AI ì‘ë‹µ ë¡œì§ (ì‹¤ì œë¡œëŠ” GPT API ì—°ê²° ì˜ˆì •)
      const aiResponse = generateAiResponse(userText)
      setConversation(prev => [...prev, { type: 'ai', text: aiResponse, timestamp: new Date() }])

      // ì—¬ê¸°ì— ì‹¤ì œ TTS API í˜¸ì¶œì´ ë“¤ì–´ê°ˆ ì˜ˆì •
      setTimeout(() => {
        setIsAiSpeaking(false)
        setCurrentStep('waiting')
      }, 2500)
    }, 1500)
  }

  const generateAiResponse = (userText) => {
    // ê°„ë‹¨í•œ ì‘ë‹µ ë¡œì§ (ì‹¤ì œë¡œëŠ” GPT APIë¡œ êµì²´ ì˜ˆì •)
    const responses = [
      "ë„¤, ì¢‹ì€ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤. ê·¸ëŸ¼ ë‹¤ìŒ ì§ˆë¬¸ë“œë¦¬ê² ìŠµë‹ˆë‹¤. í˜„ì¬ ì •ë¶€ì˜ ë³µì§€ ì •ì±…ì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ ìƒê°í•˜ì‹œë‚˜ìš”?",
      "ì˜ê²¬ ì˜ ë“¤ì—ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì •ë¶€ê°€ ìš°ì„ ì ìœ¼ë¡œ ê°œì„ í•´ì•¼ í•  ë¶€ë¶„ì´ ìˆë‹¤ë©´ ë¬´ì—‡ì´ë¼ê³  ìƒê°í•˜ì‹œë‚˜ìš”?",
      "ê·€ì¤‘í•œ ì˜ê²¬ ê°ì‚¬í•©ë‹ˆë‹¤. ì„¤ë¬¸ì¡°ì‚¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì°¸ì—¬í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤."
    ]
    return responses[Math.min(conversation.filter(c => c.type === 'ai').length, responses.length - 1)]
  }

  const stopConversation = () => {
    setCurrentStep('waiting')
    setIsListening(false)
    setIsAiSpeaking(false)
    if (recognitionRef.current) {
      recognitionRef.current.stop()
    }
  }

  const resetDemo = () => {
    setConversation([])
    setCurrentStep('waiting')
    setIsListening(false)
    setIsAiSpeaking(false)
    setTranscript('')
  }

  const getStatusText = () => {
    switch (currentStep) {
      case 'listening': return 'ğŸ¤ ë“£ê³  ìˆì–´ìš”...'
      case 'processing': return 'ğŸ¤– ë¶„ì„ ì¤‘...'
      case 'speaking': return 'ğŸ—£ï¸ AI ì‘ë‹µ ì¤‘...'
      default: return 'ëŒ€í™” ì¤€ë¹„ ì™„ë£Œ'
    }
  }

  return (
    <section id="demo" className="live-demo-section">
      <div className="container">
        <div className="demo-title">
          <h2>VOTICA AI <span className="gradient-text">ì‹¤ì œ ì‹œì—°</span></h2>
          <p>ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ AIì™€ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”</p>
        </div>

        <div className="live-demo-container">
          <div className="live-demo-card">
            <div className="demo-header">
              <div className="demo-info">
                <h3>ì‹¤ì‹œê°„ AI ìŒì„± ëŒ€í™”</h3>
                <p>AIê°€ ì§ˆë¬¸í•˜ë©´ ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”</p>
              </div>

              <div className="live-indicator">
                <div className={`live-dot ${isListening || isAiSpeaking ? 'active' : ''}`}></div>
                {getStatusText()}
              </div>
            </div>

            {/* ëŒ€í™” ê¸°ë¡ */}
            <div className="conversation-area">
              {conversation.length === 0 ? (
                <div className="conversation-placeholder">
                  <div className="placeholder-icon">ğŸ’¬</div>
                  <p>ëŒ€í™” ì‹œì‘ì„ ëˆŒëŸ¬ AIì™€ ì—¬ë¡ ì¡°ì‚¬ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”</p>
                </div>
              ) : (
                <div className="conversation-history">
                  {conversation.map((message, index) => (
                    <div key={index} className={`message ${message.type}`}>
                      <div className="message-avatar">
                        {message.type === 'ai' ? 'ğŸ¤–' : 'ğŸ‘¤'}
                      </div>
                      <div className="message-content">
                        <div className="message-text">{message.text}</div>
                        <div className="message-time">
                          {message.timestamp.toLocaleTimeString()}
                        </div>
                      </div>
                    </div>
                  ))}
                </div>
              )}

              {transcript && isListening && (
                <div className="live-transcript">
                  <span className="transcript-label">ì¸ì‹ ì¤‘:</span>
                  <span className="transcript-text">{transcript}</span>
                </div>
              )}
            </div>

            {/* ì œì–´ ë²„íŠ¼ */}
            <div className="demo-controls">
              <div className="control-buttons">
                {currentStep === 'waiting' && conversation.length === 0 && (
                  <button
                    onClick={startConversation}
                    className="demo-button primary large"
                  >
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M8 5v14l11-7z"/>
                    </svg>
                    ëŒ€í™” ì‹œì‘
                  </button>
                )}

                {currentStep === 'waiting' && conversation.length > 0 && (
                  <button
                    onClick={startListening}
                    className="demo-button mic"
                    disabled={isListening || isAiSpeaking}
                  >
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                      <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                    </svg>
                    ìŒì„±ìœ¼ë¡œ ë‹µë³€
                  </button>
                )}

                {currentStep === 'listening' && (
                  <button
                    onClick={stopConversation}
                    className="demo-button secondary"
                  >
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M6 6h12v12H6z"/>
                    </svg>
                    ì¤‘ì§€
                  </button>
                )}

                {(currentStep === 'processing' || currentStep === 'speaking') && (
                  <div className="audio-visualizer">
                    <div className="wave-container">
                      {[...Array(6)].map((_, i) => (
                        <div key={i} className="wave-bar" style={{ animationDelay: `${i * 0.1}s` }}></div>
                      ))}
                    </div>
                    <p className="playing-text">{getStatusText()}</p>
                  </div>
                )}

                {conversation.length > 0 && (
                  <button
                    onClick={resetDemo}
                    className="demo-button tertiary"
                    disabled={isListening || isAiSpeaking}
                  >
                    ìƒˆë¡œ ì‹œì‘
                  </button>
                )}
              </div>
            </div>

            <div className="demo-features">
              <div className="feature-item">
                <div className="feature-icon">ğŸ¤</div>
                <div className="feature-text">
                  <strong>Google STT</strong>
                  <span>ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜</span>
                </div>
              </div>
              <div className="feature-item">
                <div className="feature-icon">ğŸ§ </div>
                <div className="feature-text">
                  <strong>ChatGPT</strong>
                  <span>ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìƒì„±</span>
                </div>
              </div>
              <div className="feature-item">
                <div className="feature-icon">ğŸ”Š</div>
                <div className="feature-text">
                  <strong>ElevenLabs TTS</strong>
                  <span>ìì—°ìŠ¤ëŸ¬ìš´ AI ìŒì„±</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  )
}
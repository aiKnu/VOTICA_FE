'use client'

import { useState, useRef, useCallback } from 'react'

export default function LiveDemoSection() {
  const [isListening, setIsListening] = useState(false)
  const [isAiSpeaking, setIsAiSpeaking] = useState(false)
  const [conversation, setConversation] = useState([])
  const [currentStep, setCurrentStep] = useState('waiting') // waiting, listening, processing, speaking
  const [transcript, setTranscript] = useState('')
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [error, setError] = useState('')

  const mediaRecorderRef = useRef(null)
  const audioChunksRef = useRef([])
  const streamRef = useRef(null)

  // ë¸Œë¼ìš°ì €ë³„ ìµœì  MIME íƒ€ì… ì„ íƒ
  const getBestMimeType = () => {
    const types = [
      'audio/webm;codecs=opus',  // Chrome, Edge
      'audio/ogg;codecs=opus',    // Firefox
      'audio/mp4',                // Safari
      'audio/wav'                 // Fallback
    ]

    for (const type of types) {
      if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported(type)) {
        return type
      }
    }
    return 'audio/webm'
  }

  // WebM/Opusë¥¼ WAVë¡œ ë³€í™˜
  const convertToWav = async (webmBlob) => {
    const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 })
    const arrayBuffer = await webmBlob.arrayBuffer()

    try {
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)

      // WAV í—¤ë” ìƒì„±
      const length = audioBuffer.length
      const buffer = new ArrayBuffer(length * 2 + 44)
      const view = new DataView(buffer)

      // WAV í—¤ë” ì‘ì„±
      const writeString = (offset, string) => {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i))
        }
      }

      writeString(0, 'RIFF')
      view.setUint32(4, length * 2 + 36, true)
      writeString(8, 'WAVE')
      writeString(12, 'fmt ')
      view.setUint32(16, 16, true) // fmt chunk size
      view.setUint16(20, 1, true) // PCM format
      view.setUint16(22, 1, true) // mono
      view.setUint32(24, 16000, true) // sample rate
      view.setUint32(28, 32000, true) // byte rate (16000 * 2)
      view.setUint16(32, 2, true) // block align
      view.setUint16(34, 16, true) // bits per sample
      writeString(36, 'data')
      view.setUint32(40, audioBuffer.length * 2, true)

      // PCM ë°ì´í„° ì‘ì„±
      const channelData = audioBuffer.getChannelData(0)
      let offset = 44
      for (let i = 0; i < channelData.length; i++) {
        const sample = Math.max(-1, Math.min(1, channelData[i]))
        view.setInt16(offset, sample * 0x7FFF, true)
        offset += 2
      }

      return new Blob([buffer], { type: 'audio/wav' })
    } catch (error) {
      console.error('WAV ë³€í™˜ ì‹¤íŒ¨:', error)
      return webmBlob // ë³€í™˜ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    }
  }

  // ì‹¤ì œ ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    if (!streamRef.current) return

    try {
      const mimeType = getBestMimeType()
      const options = { mimeType }
      const recordingStartTime = Date.now()

      mediaRecorderRef.current = new MediaRecorder(streamRef.current, {
        ...options,
        audioBitsPerSecond: 128000
      })
      audioChunksRef.current = []

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = async () => {
        const recordingDuration = Date.now() - recordingStartTime
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })

        console.log('Audio size:', audioBlob.size, 'bytes')
        console.log('Recording duration:', recordingDuration, 'ms')

        if (audioBlob.size < 1000) {
          setError('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë…¹ìŒì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.')
          setCurrentStep('waiting')
          return
        }

        if (recordingDuration < 500) {
          setError('ë…¹ìŒ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 0.5ì´ˆ ì´ìƒ ë…¹ìŒí•´ì£¼ì„¸ìš”.')
          setCurrentStep('waiting')
          return
        }

        // ì„œë²„ë¡œ ì „ì†¡
        await sendToServer(audioBlob)
      }

      mediaRecorderRef.current.start(100)
      setIsRecording(true)
      setError('')
      console.log('ë…¹ìŒ ì‹œì‘ë¨')
    } catch (err) {
      setError('ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      console.error(err)
      setCurrentStep('waiting')
    }
  }, [])

  // ì„œë²„ë¡œ ì „ì†¡ ë° ì‘ë‹µ ì²˜ë¦¬
  const sendToServer = async (audioBlob) => {
    setCurrentStep('processing')
    setIsProcessing(true)
    setError('')

    try {
      // WAVë¡œ ë³€í™˜
      const wavBlob = await convertToWav(audioBlob)

      console.log('WAV file size:', wavBlob.size, 'bytes')

      if (wavBlob.size < 1000) {
        console.error('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤.')
        setError('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
        setIsProcessing(false)
        setCurrentStep('waiting')
        return
      }

      const formData = new FormData()
      formData.append('file', wavBlob, 'recording.wav')
      formData.append('language', 'ko-KR')

      // API í˜¸ì¶œ
      const response = await fetch('/api/voice-assistant', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        const error = await response.text()
        console.error('ì„œë²„ ì˜¤ë¥˜:', error)
        throw new Error(`ì„œë²„ ì˜¤ë¥˜: ${response.status}`)
      }

      // ìŒì„± ì‘ë‹µ ì²˜ë¦¬ (ì„œë²„ê°€ MP3 ì˜¤ë””ì˜¤ ì§ì ‘ ë°˜í™˜)
      const audioData = await response.blob()
      const audioUrl = URL.createObjectURL(audioData)

      // ë©”ì‹œì§€ ì¶”ê°€
      const userMessage = {
        type: 'user',
        text: 'ìŒì„± ì…ë ¥',
        timestamp: new Date()
      }

      const aiMessage = {
        type: 'ai',
        text: 'AI ìŒì„± ì‘ë‹µ',
        audioUrl: audioUrl,
        timestamp: new Date()
      }

      setConversation(prev => [...prev, userMessage, aiMessage])

      // ìŒì„± ìë™ ì¬ìƒ
      setCurrentStep('speaking')
      setIsAiSpeaking(true)
      const audio = new Audio(audioUrl)
      audio.onended = () => {
        setIsAiSpeaking(false)
        setCurrentStep('waiting')
      }
      await audio.play()

    } catch (err) {
      setError(err instanceof Error ? err.message : 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤')
      console.error('ì „ì†¡ ì‹¤íŒ¨:', err)
      setCurrentStep('waiting')
    } finally {
      setIsProcessing(false)
    }
  }

  const startConversation = async () => {
    // í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œë§Œ ì‹¤í–‰
    if (typeof window === 'undefined') {
      console.error('Server side - cannot access browser APIs')
      setError('ì„œë²„ ì‚¬ì´ë“œì—ì„œëŠ” ë§ˆì´í¬ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      return
    }

    // ë” ì•ˆì „í•œ navigator API ì²´í¬
    if (typeof navigator === 'undefined') {
      setError('ë¸Œë¼ìš°ì € í™˜ê²½ì´ ê°ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
      console.error('Navigator not available')
      return
    }

    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      setError('ìŒì„± ë…¹ìŒì„ ì‚¬ìš©í•˜ë ¤ë©´ HTTPS í™˜ê²½ì´ í•„ìš”í•©ë‹ˆë‹¤. http://localhost:3000 ìœ¼ë¡œ ì ‘ì†í•˜ê±°ë‚˜ HTTPSë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.')
      console.error('MediaDevices not available - HTTPS required')
      return
    }

    // HTTPS í™•ì¸ - IP ì£¼ì†ŒëŠ” localhostë¡œ ì•ˆë‚´
    const isSecureContext = location.protocol === 'https:' ||
                           location.hostname === 'localhost' ||
                           location.hostname === '127.0.0.1'

    if (!isSecureContext) {
      setError('ìŒì„± ë…¹ìŒì„ ì‚¬ìš©í•˜ë ¤ë©´ http://localhost:3000 ìœ¼ë¡œ ì ‘ì†í•˜ê±°ë‚˜ HTTPSë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.')
      console.error('Secure context required for getUserMedia')
      return
    }

    try {
      console.log('ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...')

      // ê¸°ë³¸ ì˜¤ë””ì˜¤ ì œì•½ ì¡°ê±´ (í‘œì¤€ ì†ì„±ë“¤)
      const audioConstraints = {
        channelCount: 1,        // ëª¨ë…¸
        sampleRate: 16000,      // 16kHz (Google STT ê¶Œì¥)
        sampleSize: 16,         // 16-bit
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true
      }

      // Chrome ë¸Œë¼ìš°ì €ì—ì„œë§Œ Google ì „ìš© ì†ì„± ì¶”ê°€
      const isChrome = /Chrome/.test(navigator.userAgent) && /Google Inc/.test(navigator.vendor)
      if (isChrome) {
        // Chrome ì „ìš© ì†ì„±ë“¤ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€
        Object.assign(audioConstraints, {
          googEchoCancellation: true,
          googAutoGainControl: true,
          googNoiseSuppression: true,
          googHighpassFilter: true
        })
      }

      const stream = await navigator.mediaDevices.getUserMedia({
        audio: audioConstraints
      })

      console.log('ë§ˆì´í¬ ê¶Œí•œ íšë“ ì„±ê³µ')
      streamRef.current = stream
      setError('')

      // í™˜ì˜ ë©”ì‹œì§€
      setConversation([{
        type: 'ai',
        text: 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë§ì”€í•´ì£¼ì„¸ìš”.',
        timestamp: new Date()
      }])

      // ì¦‰ì‹œ ë…¹ìŒ ì‹œì‘
      setCurrentStep('listening')
      await startRecording()

    } catch (err) {
      console.error('getUserMedia error:', err)

      if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
        setError('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
        setError('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'NotReadableError' || err.name === 'TrackStartError') {
        setError('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì•±ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'OverconstrainedError' || err.name === 'ConstraintNotSatisfiedError') {
        setError('ë§ˆì´í¬ ì„¤ì •ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'TypeError' || !navigator.mediaDevices) {
        setError('HTTPS ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. https:// ë˜ëŠ” localhostì—ì„œ ì ‘ì†í•´ì£¼ì„¸ìš”.')
      } else {
        setError('ë§ˆì´í¬ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ' + (err.message || err.name || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'))
      }
    }
  }

  const startListening = async () => {
    // ìƒˆë¡œìš´ ë…¹ìŒ ì‹œì‘
    if (streamRef.current) {
      setCurrentStep('listening')
      await startRecording()
    }
  }

  const stopConversation = () => {
    // ë…¹ìŒ ì¤‘ë‹¨í•˜ê³  API í˜¸ì¶œ
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
    } else {
      setCurrentStep('waiting')
    }
  }

  const resetDemo = () => {
    setConversation([])
    setCurrentStep('waiting')
    setIsListening(false)
    setIsAiSpeaking(false)
    setTranscript('')
  }

  const getStatusText = () => {
    if (isRecording) return 'ğŸ¤ ë…¹ìŒ ì¤‘...'
    switch (currentStep) {
      case 'listening': return 'ğŸ¤ ë“£ê³  ìˆì–´ìš”...'
      case 'processing': return 'ğŸ¤– ë¶„ì„ ì¤‘...'
      case 'speaking': return 'ğŸ—£ï¸ AI ì‘ë‹µ ì¤‘...'
      default: return 'ëŒ€í™” ì¤€ë¹„ ì™„ë£Œ'
    }
  }

  // ì •ë¦¬ í•¨ìˆ˜
  const cleanup = () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current = null
    }
    setIsRecording(false)
    setIsProcessing(false)
    setCurrentStep('waiting')
  }

  return (
    <section id="demo" className="live-demo-section">
      <div className="container">
        <div className="demo-title">
          <h2>VOTICA AI <span className="gradient-text">ì‹¤ì œ ì‹œì—°</span></h2>
          <p>ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ AIì™€ ì‹¤ì‹œê°„ ìŒì„± ëŒ€í™”ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”</p>
        </div>

        <div className="live-demo-container">
          <div className="live-demo-card">
            <div className="demo-header">
              <div className="demo-info">
                <h3>ì‹¤ì‹œê°„ AI ìŒì„± ëŒ€í™”</h3>
                <p>AIê°€ ì§ˆë¬¸í•˜ë©´ ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ìŒì„±ìœ¼ë¡œ ë‹µë³€í•´ë³´ì„¸ìš”</p>
              </div>

              <div className="live-indicator">
                <div className={`live-dot ${isRecording || isAiSpeaking ? 'active' : ''}`}></div>
                {getStatusText()}
              </div>
            </div>

            {/* ì—ëŸ¬ ë©”ì‹œì§€ */}
            {error && (
              <div className="mb-4 p-3 bg-red-500/10 border border-red-500/30 rounded-lg text-red-400 text-sm">
                {error}
              </div>
            )}

            {/* ëŒ€í™” ê¸°ë¡ */}
            <div className="conversation-area">
              {conversation.length === 0 ? (
                <div className="conversation-placeholder">
                  <div className="placeholder-icon">ğŸ’¬</div>
                  <p>ëŒ€í™” ì‹œì‘ì„ ëˆŒëŸ¬ AIì™€ ì—¬ë¡ ì¡°ì‚¬ë¥¼ ì²´í—˜í•´ë³´ì„¸ìš”</p>
                </div>
              ) : (
                <div className="conversation-history">
                  {conversation.map((message, index) => (
                    <div key={index} className={`message ${message.type}`}>
                      <div className="message-avatar">
                        {message.type === 'ai' ? 'ğŸ¤–' : 'ğŸ‘¤'}
                      </div>
                      <div className="message-content">
                        <div className="message-text">{message.text}</div>
                        {message.audioUrl && (
                          <button
                            onClick={() => {
                              const audio = new Audio(message.audioUrl)
                              audio.play()
                            }}
                            className="mt-2 p-1 rounded-lg bg-white/10 hover:bg-white/20 transition-colors"
                          >
                            ğŸ”Š ë‹¤ì‹œ ë“£ê¸°
                          </button>
                        )}
                        <div className="message-time">
                          {message.timestamp.toLocaleTimeString()}
                        </div>
                      </div>
                    </div>
                  ))}
                </div>
              )}

              {isRecording && (
                <div className="live-transcript">
                  <span className="transcript-label">ë…¹ìŒ ì¤‘:</span>
                  <span className="transcript-text">ğŸ¤ ë§ì”€í•´ì£¼ì„¸ìš”...</span>
                </div>
              )}
            </div>

            {/* ì œì–´ ë²„íŠ¼ */}
            <div className="demo-controls">
              <div className="control-buttons">
                {currentStep === 'waiting' && conversation.length === 0 && (
                  <button
                    onClick={startConversation}
                    className="demo-button primary large"
                  >
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M8 5v14l11-7z"/>
                    </svg>
                    ëŒ€í™” ì‹œì‘
                  </button>
                )}

                {currentStep === 'waiting' && conversation.length > 0 && (
                  <button
                    onClick={startListening}
                    className="demo-button mic"
                    disabled={isListening || isAiSpeaking}
                  >
                    <svg width="24" height="24" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M12 14c1.66 0 3-1.34 3-3V5c0-1.66-1.34-3-3-3S9 3.34 9 5v6c0 1.66 1.34 3 3 3z"/>
                      <path d="M17 11c0 2.76-2.24 5-5 5s-5-2.24-5-5H5c0 3.53 2.61 6.43 6 6.92V21h2v-3.08c3.39-.49 6-3.39 6-6.92h-2z"/>
                    </svg>
                    ìŒì„±ìœ¼ë¡œ ë‹µë³€
                  </button>
                )}

                {currentStep === 'listening' && (
                  <button
                    onClick={stopConversation}
                    className="demo-button secondary"
                  >
                    <svg width="20" height="20" viewBox="0 0 24 24" fill="currentColor">
                      <path d="M6 6h12v12H6z"/>
                    </svg>
                    ì¤‘ì§€
                  </button>
                )}

                {(currentStep === 'processing' || currentStep === 'speaking') && (
                  <div className="audio-visualizer">
                    <div className="wave-container">
                      {[...Array(6)].map((_, i) => (
                        <div key={i} className="wave-bar" style={{ animationDelay: `${i * 0.1}s` }}></div>
                      ))}
                    </div>
                    <p className="playing-text">{getStatusText()}</p>
                  </div>
                )}

                {conversation.length > 0 && (
                  <button
                    onClick={resetDemo}
                    className="demo-button tertiary"
                    disabled={isListening || isAiSpeaking}
                  >
                    ìƒˆë¡œ ì‹œì‘
                  </button>
                )}
              </div>
            </div>

            <div className="demo-features">
              <div className="feature-item">
                <div className="feature-icon">ğŸ¤</div>
                <div className="feature-text">
                  <strong>Google STT</strong>
                  <span>ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜</span>
                </div>
              </div>
              <div className="feature-item">
                <div className="feature-icon">ğŸ§ </div>
                <div className="feature-text">
                  <strong>ChatGPT</strong>
                  <span>ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìƒì„±</span>
                </div>
              </div>
              <div className="feature-item">
                <div className="feature-icon">ğŸ”Š</div>
                <div className="feature-text">
                  <strong>ElevenLabs TTS</strong>
                  <span>ìì—°ìŠ¤ëŸ¬ìš´ AI ìŒì„±</span>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
  )
}
'use client'

import { useState, useRef, useCallback, useEffect } from 'react'
import { motion, AnimatePresence } from 'framer-motion'
import { Mic, MicOff, Volume2, Loader2, Send, X, Play, Pause } from 'lucide-react'

interface Message {
  id: string
  type: 'user' | 'ai'
  text: string
  audioUrl?: string
  timestamp: Date
}

export default function VoiceChat() {
  const [isSessionActive, setIsSessionActive] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [messages, setMessages] = useState<Message[]>([])
  const [error, setError] = useState<string>('')
  const [isPlaying, setIsPlaying] = useState(false)
const [conversationState, setConversationState] = useState<'idle' | 'listening' | 'processing' | 'speaking'>('idle')

  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const autoRecordTimeoutRef = useRef<NodeJS.Timeout | null>(null)
  const autoStopTimeoutRef = useRef<NodeJS.Timeout | null>(null)

  // ë¸Œë¼ìš°ì €ë³„ ìµœì  MIME íƒ€ì… ì„ íƒ
  const getBestMimeType = (): string => {
    const types = [
      'audio/webm;codecs=opus',  // Chrome, Edge
      'audio/ogg;codecs=opus',    // Firefox
      'audio/mp4',                // Safari
      'audio/wav'                 // Fallback
    ]

    for (const type of types) {
      if (MediaRecorder.isTypeSupported && MediaRecorder.isTypeSupported(type)) {
        return type
      }
    }
    return 'audio/webm'
  }

  // WebM/Opusë¥¼ WAVë¡œ ë³€í™˜
  const convertToWav = async (webmBlob: Blob): Promise<Blob> => {
    const audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({ sampleRate: 16000 })
    const arrayBuffer = await webmBlob.arrayBuffer()

    try {
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer)

      // WAV í—¤ë” ìƒì„±
      const length = audioBuffer.length
      const buffer = new ArrayBuffer(length * 2 + 44)
      const view = new DataView(buffer)

      // WAV í—¤ë” ì‘ì„±
      const writeString = (offset: number, string: string) => {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i))
        }
      }

      writeString(0, 'RIFF')
      view.setUint32(4, length * 2 + 36, true)
      writeString(8, 'WAVE')
      writeString(12, 'fmt ')
      view.setUint32(16, 16, true) // fmt chunk size
      view.setUint16(20, 1, true) // PCM format
      view.setUint16(22, 1, true) // mono
      view.setUint32(24, 16000, true) // sample rate
      view.setUint32(28, 32000, true) // byte rate (16000 * 2)
      view.setUint16(32, 2, true) // block align
      view.setUint16(34, 16, true) // bits per sample
      writeString(36, 'data')
      view.setUint32(40, audioBuffer.length * 2, true)

      // PCM ë°ì´í„° ì‘ì„±
      const channelData = audioBuffer.getChannelData(0)
      let offset = 44
      for (let i = 0; i < channelData.length; i++) {
        const sample = Math.max(-1, Math.min(1, channelData[i]))
        view.setInt16(offset, sample * 0x7FFF, true)
        offset += 2
      }

      return new Blob([buffer], { type: 'audio/wav' })
    } catch (error) {
      console.error('WAV ë³€í™˜ ì‹¤íŒ¨:', error)
      return webmBlob // ë³€í™˜ ì‹¤íŒ¨ì‹œ ì›ë³¸ ë°˜í™˜
    }
  }

  // ëŒ€í™” ì„¸ì…˜ ì‹œì‘
  const startSession = async () => {
    // í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œì—ì„œë§Œ ì‹¤í–‰
    if (typeof window === 'undefined') {
      console.error('Server side - cannot access browser APIs')
      return
    }

    // navigator API ì²´í¬
    if (!navigator?.mediaDevices?.getUserMedia) {
      setError('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ë…¹ìŒì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Chrome, Firefox, ë˜ëŠ” Safarië¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.')
      console.error('getUserMedia not supported')
      return
    }

    try {
      console.log('ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...')
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          channelCount: 1,        // ëª¨ë…¸
          sampleRate: 16000,      // 16kHz (Google STT ê¶Œì¥)
          sampleSize: 16,         // 16-bit
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        }
      })

      console.log('ë§ˆì´í¬ ê¶Œí•œ íšë“ ì„±ê³µ')
      streamRef.current = stream
      setIsSessionActive(true)
      setError('')
      setConversationState('listening')

      // í™˜ì˜ ë©”ì‹œì§€
      setMessages([{
        id: Date.now().toString(),
        type: 'ai',
        text: 'ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ë§ì”€í•´ì£¼ì„¸ìš”.',
        timestamp: new Date()
      }])

      // ì¦‰ì‹œ ì²« ë²ˆì§¸ ë…¹ìŒ ì‹œì‘
      setTimeout(() => {
        startRecording()
      }, 1000) // 1ì´ˆ í›„ ìë™ ì‹œì‘
    } catch (err: any) {
      console.error('getUserMedia error:', err)

      if (err.name === 'NotAllowedError' || err.name === 'PermissionDeniedError') {
        setError('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'NotFoundError' || err.name === 'DevicesNotFoundError') {
        setError('ë§ˆì´í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë§ˆì´í¬ê°€ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'NotReadableError' || err.name === 'TrackStartError') {
        setError('ë§ˆì´í¬ê°€ ë‹¤ë¥¸ ì•±ì—ì„œ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ì•±ì„ ì¢…ë£Œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'OverconstrainedError' || err.name === 'ConstraintNotSatisfiedError') {
        setError('ë§ˆì´í¬ ì„¤ì •ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
      } else if (err.name === 'TypeError' || !navigator.mediaDevices) {
        setError('HTTPS ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤. https:// ë˜ëŠ” localhostì—ì„œ ì ‘ì†í•´ì£¼ì„¸ìš”.')
      } else {
        setError('ë§ˆì´í¬ ì ‘ê·¼ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ' + (err.message || err.name || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'))
      }
    }
  }

  // ëŒ€í™” ì„¸ì…˜ ì¢…ë£Œ
  const endSession = () => {
    // ìë™ ë…¹ìŒ íƒ€ì´ë¨¸ ì·¨ì†Œ
    cancelAutoRecording()

    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop()
      mediaRecorderRef.current = null
    }
    setIsSessionActive(false)
    setIsRecording(false)
    setIsProcessing(false)
    setConversationState('idle')
  }

  // ë…¹ìŒ ì‹œì‘
  const startRecording = useCallback(async () => {
    if (!streamRef.current) return

    // ìë™ ë…¹ìŒ íƒ€ì´ë¨¸ ì·¨ì†Œ (ìˆ˜ë™ ë…¹ìŒ ì‹œì‘ ì‹œ)
    cancelAutoRecording()

    try {
      const mimeType = getBestMimeType()
      const options: MediaRecorderOptions = { mimeType }
      const recordingStartTime = Date.now()

      mediaRecorderRef.current = new MediaRecorder(streamRef.current, {
        ...options,
        audioBitsPerSecond: 128000
      })
      audioChunksRef.current = []

      mediaRecorderRef.current.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }

      mediaRecorderRef.current.onstop = async () => {
        const recordingDuration = Date.now() - recordingStartTime
        const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })

        // íŒŒì¼ í¬ê¸° ë° ë…¹ìŒ ì‹œê°„ í™•ì¸
        console.log('Audio size:', audioBlob.size, 'bytes')
        console.log('Recording duration:', recordingDuration, 'ms')

        if (audioBlob.size < 1000) {
          setError('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë…¹ìŒì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.')
          setConversationState('listening')
          startAutoRecordingCountdown() // ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ì‹œë„
          return
        }

        if (recordingDuration < 500) {
          setError('ë…¹ìŒ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤. ìµœì†Œ 0.5ì´ˆ ì´ìƒ ë…¹ìŒí•´ì£¼ì„¸ìš”.')
          setConversationState('listening')
          startAutoRecordingCountdown() // ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ì‹œë„
          return
        }

        setConversationState('processing')
        await sendToServer(audioBlob)
      }

      mediaRecorderRef.current.start(100)
      setIsRecording(true)
      setError('')

      // 10ì´ˆ í›„ ìë™ìœ¼ë¡œ ë…¹ìŒ ì¢…ë£Œ
      autoStopTimeoutRef.current = setTimeout(() => {
        if (mediaRecorderRef.current && isRecording) {
          console.log('ìë™ ë…¹ìŒ ì¢…ë£Œ (10ì´ˆ)')
          stopRecording()
        }
      }, 10000)
    } catch (err) {
      setError('ë…¹ìŒì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
      console.error(err)
      setConversationState('listening')
      startAutoRecordingCountdown() // ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ì‹œë„
    }
  }, [])

  // ë…¹ìŒ ì¤‘ì§€
  const stopRecording = useCallback(() => {
    // ìë™ ì¢…ë£Œ íƒ€ì´ë¨¸ í´ë¦¬ì–´
    if (autoStopTimeoutRef.current) {
      clearTimeout(autoStopTimeoutRef.current)
      autoStopTimeoutRef.current = null
    }

    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
    }
  }, [isRecording])

  // ì„œë²„ë¡œ ì „ì†¡ ë° ì‘ë‹µ ì²˜ë¦¬
  const sendToServer = async (audioBlob: Blob) => {
    setIsProcessing(true)
    setError('')

    try {
      // WAVë¡œ ë³€í™˜
      const wavBlob = await convertToWav(audioBlob)

      // íŒŒì¼ í¬ê¸° í™•ì¸ (ë””ë²„ê¹…ìš©)
      console.log('WAV file size:', wavBlob.size, 'bytes')

      if (wavBlob.size < 1000) {
        console.error('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë…¹ìŒì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.')
        setError('ì˜¤ë””ì˜¤ íŒŒì¼ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.')
        setIsProcessing(false)
        return
      }

      const formData = new FormData()
      formData.append('file', wavBlob, 'recording.wav')
      formData.append('language', 'ko-KR')

      // API í˜¸ì¶œ
      const response = await fetch('/api/voice-assistant', {
        method: 'POST',
        body: formData,
      })

      if (!response.ok) {
        const error = await response.text()
        console.error('ì„œë²„ ì˜¤ë¥˜:', error)
        throw new Error(`ì„œë²„ ì˜¤ë¥˜: ${response.status}`)
      }

      // ìŒì„± ì‘ë‹µ ì²˜ë¦¬ (ì„œë²„ê°€ MP3 ì˜¤ë””ì˜¤ ì§ì ‘ ë°˜í™˜)
      const audioData = await response.blob()
      const audioUrl = URL.createObjectURL(audioData)

      // ë©”ì‹œì§€ ì¶”ê°€
      const userMessage: Message = {
        id: Date.now().toString(),
        type: 'user',
        text: 'ìŒì„± ì…ë ¥',
        timestamp: new Date()
      }

      const aiMessage: Message = {
        id: (Date.now() + 1).toString(),
        type: 'ai',
        text: 'AI ìŒì„± ì‘ë‹µ',
        audioUrl: audioUrl,
        timestamp: new Date()
      }

      setMessages(prev => [...prev, userMessage, aiMessage])

      setConversationState('processing')

      // ìŒì„± ìë™ ì¬ìƒ
      const audio = new Audio(audioUrl)
      audio.onended = () => {
        setIsPlaying(false)
        setConversationState('listening')
        // AI ì‘ë‹µ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë…¹ìŒ ì¤€ë¹„
        startAutoRecordingCountdown()
      }
      setIsPlaying(true)
      setConversationState('speaking')
      await audio.play()

    } catch (err) {
      setError(err instanceof Error ? err.message : 'ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤')
      console.error('ì „ì†¡ ì‹¤íŒ¨:', err)
      setConversationState('listening')
    } finally {
      setIsProcessing(false)
    }
  }

  // ìë™ ë…¹ìŒ ì¹´ìš´íŠ¸ë‹¤ìš´ ì‹œì‘
  const startAutoRecordingCountdown = () => {
    // ê¸°ì¡´ íƒ€ì´ë¨¸ê°€ ìˆìœ¼ë©´ í´ë¦¬ì–´
    if (autoRecordTimeoutRef.current) {
      clearTimeout(autoRecordTimeoutRef.current)
    }

    // 1ì´ˆ í›„ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë…¹ìŒ ì‹œì‘
    autoRecordTimeoutRef.current = setTimeout(() => {
      if (isSessionActive && !isRecording && !isProcessing) {
        console.log('ìë™ ë…¹ìŒ ì‹œì‘')
        startRecording()
      }
    }, 1000)
  }

  // ìë™ ë…¹ìŒ ì¹´ìš´íŠ¸ë‹¤ìš´ ì·¨ì†Œ
  const cancelAutoRecording = () => {
    if (autoRecordTimeoutRef.current) {
      clearTimeout(autoRecordTimeoutRef.current)
      autoRecordTimeoutRef.current = null
    }
  }

  // ì˜¤ë””ì˜¤ ì¬ìƒ ì œì–´
  const togglePlayAudio = (audioUrl: string) => {
    if (!audioRef.current) return

    if (audioRef.current.src === audioUrl && !audioRef.current.paused) {
      audioRef.current.pause()
      setIsPlaying(false)
    } else {
      audioRef.current.src = audioUrl
      audioRef.current.play()
      setIsPlaying(true)
    }
  }

  // ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ ì´ë²¤íŠ¸
  useEffect(() => {
    if (audioRef.current) {
      audioRef.current.onended = () => setIsPlaying(false)
    }
  }, [])

  // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ í´ë¦°ì—…
  useEffect(() => {
    return () => {
      // ëª¨ë“  íƒ€ì´ë¨¸ í´ë¦°ì—…
      cancelAutoRecording()
      if (autoStopTimeoutRef.current) {
        clearTimeout(autoStopTimeoutRef.current)
      }

      // ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
      }

      // MediaRecorder ì •ë¦¬
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
      }
    }
  }, [])

  return (
    <div className="min-h-screen bg-gradient-to-b from-gray-900 to-black">
      <div className="max-w-4xl mx-auto px-4 py-8">
        {/* í—¤ë” */}
        <motion.div
          initial={{ opacity: 0, y: -20 }}
          animate={{ opacity: 1, y: 0 }}
          className="text-center mb-8"
        >
          <h1 className="text-4xl font-bold bg-gradient-to-r from-cyan-400 to-purple-400 bg-clip-text text-transparent mb-2">
            AI ìŒì„± ëŒ€í™”
          </h1>
          <p className="text-gray-400">AIì™€ ìì—°ìŠ¤ëŸ¬ìš´ ìŒì„± ëŒ€í™”ë¥¼ ë‚˜ëˆ ë³´ì„¸ìš”</p>
        </motion.div>

        {/* ë©”ì¸ ëŒ€í™” ì˜ì—­ */}
        <motion.div
          initial={{ opacity: 0, scale: 0.95 }}
          animate={{ opacity: 1, scale: 1 }}
          className="bg-gray-900/50 backdrop-blur-lg rounded-2xl border border-gray-800 p-6"
        >
          {!isSessionActive ? (
            /* ì‹œì‘ í™”ë©´ */
            <div className="text-center py-20">
              <motion.div
                initial={{ scale: 0 }}
                animate={{ scale: 1 }}
                transition={{ type: "spring", duration: 0.5 }}
                className="inline-flex items-center justify-center w-32 h-32 rounded-full bg-gradient-to-br from-cyan-500 to-purple-500 mb-6"
              >
                <Mic className="w-16 h-16 text-white" />
              </motion.div>

              <h2 className="text-2xl font-semibold mb-4">ëŒ€í™”ë¥¼ ì‹œì‘í•˜ì‹œê² ìŠµë‹ˆê¹Œ?</h2>
              <p className="text-gray-400 mb-8">ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤</p>

              <motion.button
                whileHover={{ scale: 1.05 }}
                whileTap={{ scale: 0.95 }}
                onClick={startSession}
                className="px-8 py-4 bg-gradient-to-r from-cyan-500 to-purple-500 rounded-xl text-white font-semibold shadow-lg hover:shadow-cyan-500/25 transition-all"
              >
                ëŒ€í™” ì‹œì‘
              </motion.button>
            </div>
          ) : (
            /* ëŒ€í™” í™”ë©´ */
            <div>
              {/* ëŒ€í™” ë‚´ì—­ */}
              <div className="h-[400px] overflow-y-auto mb-6 space-y-4 pr-2 custom-scrollbar">
                <AnimatePresence>
                  {messages.map((message) => (
                    <motion.div
                      key={message.id}
                      initial={{ opacity: 0, y: 20 }}
                      animate={{ opacity: 1, y: 0 }}
                      exit={{ opacity: 0, y: -20 }}
                      className={`flex ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}
                    >
                      <div className={`max-w-[70%] ${message.type === 'user' ? 'order-1' : ''}`}>
                        <div className={`rounded-2xl px-4 py-3 ${
                          message.type === 'user'
                            ? 'bg-gradient-to-r from-cyan-600 to-blue-600 text-white'
                            : 'bg-gray-800 text-gray-100'
                        }`}>
                          <p className="text-sm font-medium mb-1">
                            {message.type === 'user' ? 'ë‚˜' : 'AI'}
                          </p>
                          <p>{message.text}</p>

                          {message.audioUrl && (
                            <button
                              onClick={() => togglePlayAudio(message.audioUrl!)}
                              className="mt-2 p-1 rounded-lg bg-white/10 hover:bg-white/20 transition-colors"
                            >
                              {isPlaying && audioRef.current?.src === message.audioUrl ? (
                                <Pause className="w-4 h-4" />
                              ) : (
                                <Play className="w-4 h-4" />
                              )}
                            </button>
                          )}
                        </div>
                        <p className="text-xs text-gray-500 mt-1 px-2">
                          {message.timestamp.toLocaleTimeString()}
                        </p>
                      </div>
                    </motion.div>
                  ))}
                </AnimatePresence>

                {isProcessing && (
                  <motion.div
                    initial={{ opacity: 0 }}
                    animate={{ opacity: 1 }}
                    className="flex justify-start"
                  >
                    <div className="bg-gray-800 rounded-2xl px-4 py-3 flex items-center space-x-2">
                      <Loader2 className="w-4 h-4 animate-spin text-cyan-400" />
                      <span className="text-gray-400">AIê°€ ìƒê°ì¤‘...</span>
                    </div>
                  </motion.div>
                )}
              </div>

              {/* ì—ëŸ¬ ë©”ì‹œì§€ */}
              {error && (
                <motion.div
                  initial={{ opacity: 0, y: -10 }}
                  animate={{ opacity: 1, y: 0 }}
                  className="mb-4 p-3 bg-red-500/10 border border-red-500/30 rounded-lg text-red-400 text-sm"
                >
                  {error}
                </motion.div>
              )}

              {/* ìƒíƒœ í‘œì‹œ ë° ì»¨íŠ¸ë¡¤ */}
              <div className="text-center space-y-4">
                {/* ìƒíƒœ ì‹œê°ì  í‘œì‹œ */}
                <div className="flex items-center justify-center">
                  {conversationState === 'listening' && (
                    <motion.div
                      animate={{ scale: [1, 1.2, 1] }}
                      transition={{ repeat: Infinity, duration: 2 }}
                      className="w-16 h-16 rounded-full bg-gradient-to-r from-cyan-500 to-blue-500 flex items-center justify-center"
                    >
                      <Volume2 className="w-8 h-8 text-white" />
                    </motion.div>
                  )}

                  {isRecording && (
                    <motion.div
                      animate={{ scale: [1, 1.1, 1] }}
                      transition={{ repeat: Infinity, duration: 1 }}
                      className="w-16 h-16 rounded-full bg-red-500 flex items-center justify-center"
                    >
                      <Mic className="w-8 h-8 text-white" />
                    </motion.div>
                  )}

                  {conversationState === 'processing' && (
                    <div className="w-16 h-16 rounded-full bg-gradient-to-r from-purple-500 to-pink-500 flex items-center justify-center">
                      <Loader2 className="w-8 h-8 text-white animate-spin" />
                    </div>
                  )}

                  {conversationState === 'speaking' && (
                    <motion.div
                      animate={{ scale: [1, 1.1, 1] }}
                      transition={{ repeat: Infinity, duration: 1.5 }}
                      className="w-16 h-16 rounded-full bg-gradient-to-r from-green-500 to-emerald-500 flex items-center justify-center"
                    >
                      <Volume2 className="w-8 h-8 text-white" />
                    </motion.div>
                  )}
                </div>

                {/* ëŒ€í™” ì¢…ë£Œ ë²„íŠ¼ */}
                <motion.button
                  whileHover={{ scale: 1.05 }}
                  whileTap={{ scale: 0.95 }}
                  onClick={endSession}
                  className="px-8 py-3 bg-gray-800 hover:bg-gray-700 rounded-xl text-white font-medium transition-all"
                >
                  ëŒ€í™” ì¢…ë£Œ
                </motion.button>
              </div>

              <p className="text-center text-gray-500 text-sm mt-4">
                {conversationState === 'listening' && 'ğŸ§ ì ì‹œ í›„ ë…¹ìŒì´ ì‹œì‘ë©ë‹ˆë‹¤...'}
                {isRecording && 'ğŸ™ï¸ ë…¹ìŒ ì¤‘... ë§ì”€í•´ì£¼ì„¸ìš” (ìµœëŒ€ 10ì´ˆ)'}
                {conversationState === 'processing' && 'ğŸ¤– AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}
                {conversationState === 'speaking' && 'ğŸ”Š AIê°€ ë‹µë³€í•˜ê³  ìˆìŠµë‹ˆë‹¤...'}
                {conversationState === 'idle' && 'ëŒ€í™”ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”'}
              </p>
            </div>
          )}
        </motion.div>

        {/* ì‚¬ìš© ì•ˆë‚´ */}
        <motion.div
          initial={{ opacity: 0 }}
          animate={{ opacity: 1 }}
          transition={{ delay: 0.3 }}
          className="mt-8 text-center text-gray-400 text-sm"
        >
          <p>ğŸ’¡ íŒ: ì¡°ìš©í•œ í™˜ê²½ì—ì„œ ëª…í™•í•˜ê²Œ ë§ì”€í•´ì£¼ì‹œë©´ ë” ì •í™•í•œ ì¸ì‹ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤</p>
        </motion.div>
      </div>

      {/* ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ (ìˆ¨ê¹€) */}
      <audio ref={audioRef} className="hidden" />

      <style jsx>{`
        .custom-scrollbar::-webkit-scrollbar {
          width: 6px;
        }
        .custom-scrollbar::-webkit-scrollbar-track {
          background: rgba(0, 0, 0, 0.2);
          border-radius: 10px;
        }
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: linear-gradient(180deg, #06b6d4, #a855f7);
          border-radius: 10px;
        }
      `}</style>
    </div>
  )
}